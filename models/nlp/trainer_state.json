{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 2344,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.021340162185232606,
      "grad_norm": 5.914109230041504,
      "learning_rate": 1.9795221843003412e-05,
      "loss": 0.7944,
      "step": 25
    },
    {
      "epoch": 0.04268032437046521,
      "grad_norm": 7.650694370269775,
      "learning_rate": 1.9581911262798636e-05,
      "loss": 0.838,
      "step": 50
    },
    {
      "epoch": 0.06402048655569782,
      "grad_norm": 18.1055908203125,
      "learning_rate": 1.936860068259386e-05,
      "loss": 0.5815,
      "step": 75
    },
    {
      "epoch": 0.08536064874093043,
      "grad_norm": 2.430579662322998,
      "learning_rate": 1.915529010238908e-05,
      "loss": 0.8049,
      "step": 100
    },
    {
      "epoch": 0.10670081092616304,
      "grad_norm": 1.7655627727508545,
      "learning_rate": 1.89419795221843e-05,
      "loss": 0.9086,
      "step": 125
    },
    {
      "epoch": 0.12804097311139565,
      "grad_norm": 2.3773200511932373,
      "learning_rate": 1.8728668941979525e-05,
      "loss": 0.7209,
      "step": 150
    },
    {
      "epoch": 0.14938113529662825,
      "grad_norm": 3.5237903594970703,
      "learning_rate": 1.8515358361774745e-05,
      "loss": 0.6268,
      "step": 175
    },
    {
      "epoch": 0.17072129748186085,
      "grad_norm": 4.946468830108643,
      "learning_rate": 1.8302047781569966e-05,
      "loss": 0.8845,
      "step": 200
    },
    {
      "epoch": 0.19206145966709348,
      "grad_norm": 5.954342842102051,
      "learning_rate": 1.808873720136519e-05,
      "loss": 0.8605,
      "step": 225
    },
    {
      "epoch": 0.21340162185232608,
      "grad_norm": 4.81350040435791,
      "learning_rate": 1.787542662116041e-05,
      "loss": 0.5245,
      "step": 250
    },
    {
      "epoch": 0.2347417840375587,
      "grad_norm": 13.664165496826172,
      "learning_rate": 1.7662116040955634e-05,
      "loss": 0.7388,
      "step": 275
    },
    {
      "epoch": 0.2560819462227913,
      "grad_norm": 3.9173965454101562,
      "learning_rate": 1.7448805460750855e-05,
      "loss": 0.8321,
      "step": 300
    },
    {
      "epoch": 0.2774221084080239,
      "grad_norm": 3.1355719566345215,
      "learning_rate": 1.723549488054608e-05,
      "loss": 0.7967,
      "step": 325
    },
    {
      "epoch": 0.2987622705932565,
      "grad_norm": 21.21381950378418,
      "learning_rate": 1.70221843003413e-05,
      "loss": 0.5879,
      "step": 350
    },
    {
      "epoch": 0.3201024327784891,
      "grad_norm": 6.6930131912231445,
      "learning_rate": 1.680887372013652e-05,
      "loss": 0.7352,
      "step": 375
    },
    {
      "epoch": 0.3414425949637217,
      "grad_norm": 3.639742136001587,
      "learning_rate": 1.6595563139931743e-05,
      "loss": 0.4952,
      "step": 400
    },
    {
      "epoch": 0.3627827571489543,
      "grad_norm": 12.332503318786621,
      "learning_rate": 1.6382252559726964e-05,
      "loss": 0.5804,
      "step": 425
    },
    {
      "epoch": 0.38412291933418696,
      "grad_norm": 2.054060220718384,
      "learning_rate": 1.6168941979522184e-05,
      "loss": 0.5007,
      "step": 450
    },
    {
      "epoch": 0.40546308151941957,
      "grad_norm": 2.431147575378418,
      "learning_rate": 1.5955631399317408e-05,
      "loss": 0.4862,
      "step": 475
    },
    {
      "epoch": 0.42680324370465217,
      "grad_norm": 4.750767707824707,
      "learning_rate": 1.574232081911263e-05,
      "loss": 0.5287,
      "step": 500
    },
    {
      "epoch": 0.44814340588988477,
      "grad_norm": 9.797295570373535,
      "learning_rate": 1.5529010238907852e-05,
      "loss": 0.5666,
      "step": 525
    },
    {
      "epoch": 0.4694835680751174,
      "grad_norm": 4.68347692489624,
      "learning_rate": 1.5315699658703073e-05,
      "loss": 0.768,
      "step": 550
    },
    {
      "epoch": 0.49082373026035,
      "grad_norm": 9.874897003173828,
      "learning_rate": 1.5102389078498293e-05,
      "loss": 0.5683,
      "step": 575
    },
    {
      "epoch": 0.5121638924455826,
      "grad_norm": 23.97722816467285,
      "learning_rate": 1.4889078498293517e-05,
      "loss": 0.5084,
      "step": 600
    },
    {
      "epoch": 0.5335040546308152,
      "grad_norm": 13.412481307983398,
      "learning_rate": 1.467576791808874e-05,
      "loss": 0.4299,
      "step": 625
    },
    {
      "epoch": 0.5548442168160478,
      "grad_norm": 16.862581253051758,
      "learning_rate": 1.446245733788396e-05,
      "loss": 0.4677,
      "step": 650
    },
    {
      "epoch": 0.5761843790012804,
      "grad_norm": 11.044754981994629,
      "learning_rate": 1.4249146757679182e-05,
      "loss": 0.4404,
      "step": 675
    },
    {
      "epoch": 0.597524541186513,
      "grad_norm": 6.456369400024414,
      "learning_rate": 1.4035836177474404e-05,
      "loss": 0.6376,
      "step": 700
    },
    {
      "epoch": 0.6188647033717456,
      "grad_norm": 64.50733947753906,
      "learning_rate": 1.3822525597269625e-05,
      "loss": 0.6342,
      "step": 725
    },
    {
      "epoch": 0.6402048655569782,
      "grad_norm": 12.07484245300293,
      "learning_rate": 1.3609215017064847e-05,
      "loss": 0.4726,
      "step": 750
    },
    {
      "epoch": 0.6615450277422108,
      "grad_norm": 28.488515853881836,
      "learning_rate": 1.3395904436860069e-05,
      "loss": 0.4397,
      "step": 775
    },
    {
      "epoch": 0.6828851899274434,
      "grad_norm": 25.852991104125977,
      "learning_rate": 1.318259385665529e-05,
      "loss": 0.5485,
      "step": 800
    },
    {
      "epoch": 0.704225352112676,
      "grad_norm": 1.5969102382659912,
      "learning_rate": 1.2969283276450513e-05,
      "loss": 0.4257,
      "step": 825
    },
    {
      "epoch": 0.7255655142979086,
      "grad_norm": 6.97193717956543,
      "learning_rate": 1.2755972696245736e-05,
      "loss": 1.0241,
      "step": 850
    },
    {
      "epoch": 0.7469056764831413,
      "grad_norm": 16.73926544189453,
      "learning_rate": 1.2542662116040956e-05,
      "loss": 0.3254,
      "step": 875
    },
    {
      "epoch": 0.7682458386683739,
      "grad_norm": 17.559680938720703,
      "learning_rate": 1.2329351535836178e-05,
      "loss": 0.7663,
      "step": 900
    },
    {
      "epoch": 0.7895860008536065,
      "grad_norm": 18.56330680847168,
      "learning_rate": 1.21160409556314e-05,
      "loss": 0.682,
      "step": 925
    },
    {
      "epoch": 0.8109261630388391,
      "grad_norm": 1.3209623098373413,
      "learning_rate": 1.1902730375426623e-05,
      "loss": 0.6303,
      "step": 950
    },
    {
      "epoch": 0.8322663252240717,
      "grad_norm": 15.103610038757324,
      "learning_rate": 1.1689419795221843e-05,
      "loss": 0.5687,
      "step": 975
    },
    {
      "epoch": 0.8536064874093043,
      "grad_norm": 3.216477394104004,
      "learning_rate": 1.1476109215017065e-05,
      "loss": 0.4494,
      "step": 1000
    },
    {
      "epoch": 0.8749466495945369,
      "grad_norm": 3.4491279125213623,
      "learning_rate": 1.126279863481229e-05,
      "loss": 0.4232,
      "step": 1025
    },
    {
      "epoch": 0.8962868117797695,
      "grad_norm": 10.907252311706543,
      "learning_rate": 1.104948805460751e-05,
      "loss": 0.62,
      "step": 1050
    },
    {
      "epoch": 0.9176269739650021,
      "grad_norm": 6.41345739364624,
      "learning_rate": 1.0836177474402732e-05,
      "loss": 0.7864,
      "step": 1075
    },
    {
      "epoch": 0.9389671361502347,
      "grad_norm": 21.891700744628906,
      "learning_rate": 1.0622866894197954e-05,
      "loss": 0.5256,
      "step": 1100
    },
    {
      "epoch": 0.9603072983354674,
      "grad_norm": 25.1335506439209,
      "learning_rate": 1.0409556313993175e-05,
      "loss": 0.4458,
      "step": 1125
    },
    {
      "epoch": 0.9816474605207,
      "grad_norm": 14.765984535217285,
      "learning_rate": 1.0196245733788397e-05,
      "loss": 0.3857,
      "step": 1150
    },
    {
      "epoch": 1.002560819462228,
      "grad_norm": 5.806451797485352,
      "learning_rate": 9.982935153583619e-06,
      "loss": 0.5091,
      "step": 1175
    },
    {
      "epoch": 1.0239009816474605,
      "grad_norm": 1.4958630800247192,
      "learning_rate": 9.769624573378841e-06,
      "loss": 0.6369,
      "step": 1200
    },
    {
      "epoch": 1.0452411438326932,
      "grad_norm": 9.772640228271484,
      "learning_rate": 9.556313993174062e-06,
      "loss": 0.3732,
      "step": 1225
    },
    {
      "epoch": 1.0665813060179257,
      "grad_norm": 30.718753814697266,
      "learning_rate": 9.343003412969284e-06,
      "loss": 0.3943,
      "step": 1250
    },
    {
      "epoch": 1.0879214682031584,
      "grad_norm": 21.771804809570312,
      "learning_rate": 9.129692832764506e-06,
      "loss": 0.2723,
      "step": 1275
    },
    {
      "epoch": 1.1092616303883909,
      "grad_norm": 2.966359853744507,
      "learning_rate": 8.916382252559728e-06,
      "loss": 0.5073,
      "step": 1300
    },
    {
      "epoch": 1.1306017925736236,
      "grad_norm": 1.5849822759628296,
      "learning_rate": 8.703071672354949e-06,
      "loss": 0.3085,
      "step": 1325
    },
    {
      "epoch": 1.151941954758856,
      "grad_norm": 10.911582946777344,
      "learning_rate": 8.48976109215017e-06,
      "loss": 0.3854,
      "step": 1350
    },
    {
      "epoch": 1.1732821169440888,
      "grad_norm": 10.068986892700195,
      "learning_rate": 8.276450511945393e-06,
      "loss": 0.5504,
      "step": 1375
    },
    {
      "epoch": 1.1946222791293213,
      "grad_norm": 39.36089324951172,
      "learning_rate": 8.063139931740615e-06,
      "loss": 0.4033,
      "step": 1400
    },
    {
      "epoch": 1.215962441314554,
      "grad_norm": 20.900606155395508,
      "learning_rate": 7.849829351535837e-06,
      "loss": 0.5481,
      "step": 1425
    },
    {
      "epoch": 1.2373026034997867,
      "grad_norm": 1.0770257711410522,
      "learning_rate": 7.636518771331058e-06,
      "loss": 0.2955,
      "step": 1450
    },
    {
      "epoch": 1.2586427656850192,
      "grad_norm": 780.3085327148438,
      "learning_rate": 7.423208191126281e-06,
      "loss": 0.4178,
      "step": 1475
    },
    {
      "epoch": 1.2799829278702517,
      "grad_norm": 13.72724437713623,
      "learning_rate": 7.209897610921502e-06,
      "loss": 0.257,
      "step": 1500
    },
    {
      "epoch": 1.3013230900554844,
      "grad_norm": 8.921881675720215,
      "learning_rate": 6.9965870307167235e-06,
      "loss": 0.5749,
      "step": 1525
    },
    {
      "epoch": 1.3226632522407171,
      "grad_norm": 20.227676391601562,
      "learning_rate": 6.7832764505119465e-06,
      "loss": 0.646,
      "step": 1550
    },
    {
      "epoch": 1.3440034144259496,
      "grad_norm": 6.662471771240234,
      "learning_rate": 6.569965870307168e-06,
      "loss": 0.4117,
      "step": 1575
    },
    {
      "epoch": 1.365343576611182,
      "grad_norm": 0.5068186521530151,
      "learning_rate": 6.356655290102389e-06,
      "loss": 0.3405,
      "step": 1600
    },
    {
      "epoch": 1.3866837387964148,
      "grad_norm": 16.85029411315918,
      "learning_rate": 6.143344709897611e-06,
      "loss": 0.3825,
      "step": 1625
    },
    {
      "epoch": 1.4080239009816475,
      "grad_norm": 15.690350532531738,
      "learning_rate": 5.9300341296928336e-06,
      "loss": 0.3825,
      "step": 1650
    },
    {
      "epoch": 1.42936406316688,
      "grad_norm": 9.298774719238281,
      "learning_rate": 5.716723549488055e-06,
      "loss": 0.2863,
      "step": 1675
    },
    {
      "epoch": 1.4507042253521127,
      "grad_norm": 19.742834091186523,
      "learning_rate": 5.503412969283277e-06,
      "loss": 0.2226,
      "step": 1700
    },
    {
      "epoch": 1.4720443875373452,
      "grad_norm": 0.6433538198471069,
      "learning_rate": 5.290102389078498e-06,
      "loss": 0.2979,
      "step": 1725
    },
    {
      "epoch": 1.493384549722578,
      "grad_norm": 13.03191089630127,
      "learning_rate": 5.07679180887372e-06,
      "loss": 0.4942,
      "step": 1750
    },
    {
      "epoch": 1.5147247119078107,
      "grad_norm": 7.9334821701049805,
      "learning_rate": 4.863481228668943e-06,
      "loss": 0.6574,
      "step": 1775
    },
    {
      "epoch": 1.5360648740930432,
      "grad_norm": 0.6099480390548706,
      "learning_rate": 4.650170648464164e-06,
      "loss": 0.5978,
      "step": 1800
    },
    {
      "epoch": 1.5574050362782756,
      "grad_norm": 22.795629501342773,
      "learning_rate": 4.436860068259386e-06,
      "loss": 0.4268,
      "step": 1825
    },
    {
      "epoch": 1.5787451984635084,
      "grad_norm": 40.67654037475586,
      "learning_rate": 4.223549488054608e-06,
      "loss": 0.4623,
      "step": 1850
    },
    {
      "epoch": 1.600085360648741,
      "grad_norm": 4.173203468322754,
      "learning_rate": 4.01023890784983e-06,
      "loss": 0.5549,
      "step": 1875
    },
    {
      "epoch": 1.6214255228339736,
      "grad_norm": 9.597521781921387,
      "learning_rate": 3.7969283276450516e-06,
      "loss": 0.3325,
      "step": 1900
    },
    {
      "epoch": 1.642765685019206,
      "grad_norm": 9.281848907470703,
      "learning_rate": 3.5836177474402733e-06,
      "loss": 0.27,
      "step": 1925
    },
    {
      "epoch": 1.6641058472044388,
      "grad_norm": 46.081424713134766,
      "learning_rate": 3.3703071672354955e-06,
      "loss": 0.5951,
      "step": 1950
    },
    {
      "epoch": 1.6854460093896715,
      "grad_norm": 0.1867905706167221,
      "learning_rate": 3.156996587030717e-06,
      "loss": 0.3516,
      "step": 1975
    },
    {
      "epoch": 1.706786171574904,
      "grad_norm": 34.99664306640625,
      "learning_rate": 2.9436860068259386e-06,
      "loss": 0.4439,
      "step": 2000
    },
    {
      "epoch": 1.7281263337601365,
      "grad_norm": 10.314187049865723,
      "learning_rate": 2.7303754266211608e-06,
      "loss": 0.3857,
      "step": 2025
    },
    {
      "epoch": 1.7494664959453692,
      "grad_norm": 15.613598823547363,
      "learning_rate": 2.5170648464163825e-06,
      "loss": 0.3124,
      "step": 2050
    },
    {
      "epoch": 1.770806658130602,
      "grad_norm": 6.523159503936768,
      "learning_rate": 2.3037542662116043e-06,
      "loss": 0.2492,
      "step": 2075
    },
    {
      "epoch": 1.7921468203158344,
      "grad_norm": 19.755537033081055,
      "learning_rate": 2.090443686006826e-06,
      "loss": 0.3061,
      "step": 2100
    },
    {
      "epoch": 1.8134869825010669,
      "grad_norm": 1.4678940773010254,
      "learning_rate": 1.8771331058020478e-06,
      "loss": 0.2781,
      "step": 2125
    },
    {
      "epoch": 1.8348271446862996,
      "grad_norm": 0.21915572881698608,
      "learning_rate": 1.6638225255972698e-06,
      "loss": 0.3235,
      "step": 2150
    },
    {
      "epoch": 1.8561673068715323,
      "grad_norm": 0.29875507950782776,
      "learning_rate": 1.4505119453924915e-06,
      "loss": 0.3721,
      "step": 2175
    },
    {
      "epoch": 1.8775074690567648,
      "grad_norm": 17.73639678955078,
      "learning_rate": 1.2372013651877133e-06,
      "loss": 0.5142,
      "step": 2200
    },
    {
      "epoch": 1.8988476312419973,
      "grad_norm": 17.521121978759766,
      "learning_rate": 1.0238907849829352e-06,
      "loss": 0.3277,
      "step": 2225
    },
    {
      "epoch": 1.92018779342723,
      "grad_norm": 5.025922775268555,
      "learning_rate": 8.105802047781571e-07,
      "loss": 0.5661,
      "step": 2250
    },
    {
      "epoch": 1.9415279556124627,
      "grad_norm": 8.87687873840332,
      "learning_rate": 5.972696245733788e-07,
      "loss": 0.5939,
      "step": 2275
    },
    {
      "epoch": 1.9628681177976952,
      "grad_norm": 12.5040922164917,
      "learning_rate": 3.839590443686007e-07,
      "loss": 0.3206,
      "step": 2300
    },
    {
      "epoch": 1.984208279982928,
      "grad_norm": 6.102189064025879,
      "learning_rate": 1.7064846416382255e-07,
      "loss": 0.51,
      "step": 2325
    }
  ],
  "logging_steps": 25,
  "max_steps": 2344,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 616359782568960.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
